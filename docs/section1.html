<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Convoy Machine Learning</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to external CSS file -->
</head>
<body>
    
    <!-- Header Section -->
    <header>
        <h1>Convoy Machine Learning</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li> <!-- Link back to homepage -->
                <li><a href="section2.html">Convoy Data Visualization</a></li> <!-- Link to Section 2 Page -->
                <li><a href="section3.html">Convoy History</a></li> <!-- Link to Section 3 Page -->
                <li><a href="section4.html">Original Project</a></li> <!-- Link to Section 4 Page -->
                <li><a href="section5.html">Additional Info</a></li> <!-- Link to Section 5 Page -->
            </ul>
        </nav>
    </header>

    <main>
        
<!-- Original Classification Results -->
        <h2 style="text-align: center;"> </strong>Exploring Classification Algorithms</h2>
        <p class="centered-paragraph">
        

        Early experiments utilized a regression approach to predict exact sink percentages. The results for these initial attempts 
        can be found <a href="section1_1.html">here.</a> The limited data, wide variability, and unmodeled factors 


        This method quickly proved to be far too difficult largely due to limited data and widely varying sink percentages.
        Instead, sink percentages were converted to binary values of 0 (no ships sunk) or 1 (at least one ship sunk). 
        This new method aims to alleviate the complexity of predicting an exact sink percentage while also providing
        a more realistic approach of identifying high-risk convoys. Various algorithms were tested to 
        determine the one best suited for the challenge.
        
    
        </p>
        
        <div class="table-container">
            <table>
                <caption>Classification Algorithm Comparison:</caption>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Acc</th>
                        <th>ROC_AUC</th>
                        <th>MCC</th>
                        <th>Bal_Acc</th>
                        <th>Recall1</th>
                        <th>F1_1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>LogisticRegression</td><td>0.79</td><td>0.786</td><td>0.193</td><td>0.554</td><td>0.14</td><td>0.22</td></tr>
                    <tr><td>SGDClassifier</td><td>0.79</td><td>0.728</td><td>0.148</td><td>0.537</td><td>0.10</td><td>0.17</td></tr>
                    <tr><td>LinearSVC</td><td>0.79</td><td>0.535</td><td>0.000</td><td>0.500</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>SVC</td><td>0.80</td><td>0.768</td><td>0.215</td><td>0.545</td><td>0.10</td><td>0.18</td></tr>
                    <tr><td>NuSVC</td><td>0.77</td><td>0.667</td><td>0.241</td><td>0.606</td><td>0.32</td><td>0.37</td></tr>
                    <tr><td>KNeighborsClassifier</td><td>0.80</td><td>0.740</td><td>0.265</td><td>0.588</td><td>0.22</td><td>0.32</td></tr>
                    <tr><td>RadiusNeighborsClassifier</td><td>0.78</td><td>0.567</td><td>-0.034</td><td>0.497</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>DecisionTreeClassifier</td><td>0.74</td><td>0.621</td><td>0.235</td><td>0.621</td><td>0.42</td><td>0.40</td></tr>
                    <tr><td>RandomForestClassifier</td><td>0.84</td><td>0.815</td><td>0.461</td><td>0.674</td><td>0.38</td><td>0.51</td></tr>
                    <tr><td>ExtraTreesClassifier</td><td>0.84</td><td>0.791</td><td>0.469</td><td>0.688</td><td>0.42</td><td>0.53</td></tr>
                    <tr><td>BaggingClassifier</td><td>0.83</td><td>0.766</td><td>0.400</td><td>0.656</td><td>0.36</td><td>0.47</td></tr>
                    <tr class="highlight-row"><td>GradientBoostingClassifier</td><td>0.84</td><td>0.835</td><td>0.473</td><td>0.696</td><td>0.44</td><td>0.54</td></tr>
                    <tr><td>AdaBoostClassifier</td><td>0.82</td><td>0.790</td><td>0.368</td><td>0.631</td><td>0.30</td><td>0.42</td></tr>
                    <tr><td>GaussianNB</td><td>0.77</td><td>0.670</td><td>0.096</td><td>0.530</td><td>0.12</td><td>0.18</td></tr>
                    <tr><td>BernoulliNB</td><td>0.79</td><td>0.699</td><td>0.000</td><td>0.500</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>ComplementNB</td><td>0.49</td><td>0.555</td><td>0.061</td><td>0.537</td><td>0.62</td><td>0.34</td></tr>
                    <tr><td>LinearDiscriminantAnalysis</td><td>0.79</td><td>0.787</td><td>0.205</td><td>0.561</td><td>0.16</td><td>0.25</td></tr>
                    <tr><td>QuadraticDiscriminantAnalysis</td><td>0.78</td><td>0.790</td><td>0.425</td><td>0.735</td><td>0.66</td><td>0.56</td></tr>
                    <tr><td>MLPClassifier</td><td>0.67</td><td>0.542</td><td>-0.064</td><td>0.471</td><td>0.12</td><td>0.13</td></tr>
                </tbody>
            </table>
        </div>
        <p class="centered-paragraph">
        The Gradient Boosting Classifier performs the best on this data and will be the foundation for all future tests. <br>
         Notably, the ComplementNB and QuadraticDiscriminantAnalysis algorithms performed comparable, even resulting in higher recall scores.  <br>
        Other Gradient Boosting algorithms from the XGBoost library will be tested next. These results are below.


        </p>

        <div class="table-container">
            <table>
                <caption>Gradient Boosting Family Comparison:</caption>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Acc</th>
                        <th>ROC_AUC</th>
                        <th>MCC</th>
                        <th>Bal_Acc</th>
                        <th>Recall1</th>
                        <th>F1_1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight-row"><td>GradientBoostingClassifier</td><td>0.84</td><td>0.835</td><td>0.473</td><td>0.696</td><td>0.44</td><td>0.54</td></tr>
                    <tr><td>XGBClassifier</td><td>0.83</td><td>0.773</td><td>0.408</td><td>0.651</td><td>0.34</td><td>0.46</td></tr>
                    <tr><td>LGBMClassifier</td><td>0.83</td><td>0.792</td><td>0.443</td><td>0.683</td><td>0.42</td><td>0.52</td></tr>
                    <tr><td>CatBoostClassifier</td><td>0.84</td><td>0.832</td><td>0.457</td><td>0.659</td><td>0.34</td><td>0.48</td></tr>
                </tbody>
            </table>
        </div>

        <p class="centered-paragraph">
            The regular Scikit-Learn Gradient Boosting edges out the other boosted tree variants by pairing the top ROC-AUC 
            and F1 scores with the strongest recall for at-risk convoys. Further optimization will now be done with focus put on recall and false negatives. 
        </p>

        <!-- Gradient Boosting Classifier Detailed Results -->
    <h2 style="text-align: center;">Gradient Boosting Classifier Results</h2>
    <div class="table-container">
        <table>
            <tr><th colspan="2">Gradient Boosting Evaluation</th></tr>
            <tr><td>ROC AUC Score</td><td>0.7955</td></tr>
            <tr><td>Matthews Correlation Coefficient (MCC)</td><td>0.4605</td></tr>
            <tr><td>Balanced Accuracy</td><td>0.6930</td></tr>
            <tr><td>Recall (positive=1)</td><td>0.4400</td></tr>
            <tr><td>F2 Score</td><td>0.4741</td></tr>
            <tr><td>False Negatives</td><td>28</td></tr>
        </table>
    </div>

    <div class="table-container">
        <table>
            <caption>Gradient Boosting Classification Report</caption>
            <thead>
                <tr>
                    <th></th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>0 (No Risk)</td><td>0.86</td><td>0.95</td><td>0.90</td><td>185</td></tr>
                <tr><td>1 (At Risk)</td><td>0.69</td><td>0.44</td><td>0.54</td><td>50</td></tr>
                <tr><td>Macro Avg</td><td>0.77</td><td>0.69</td><td>0.72</td><td>235</td></tr>
                <tr><td>Weighted Avg</td><td>0.82</td><td>0.84</td><td>0.82</td><td>235</td></tr>
                <tr><td>Accuracy</td><td colspan="4">0.84</td></tr>
            </tbody>
        </table>
    </div>

    <div class="table-container">
        <table>
            <caption>Gradient Boosting Threshold Comparison</caption>
            <thead>
                <tr>
                    <th>Threshold</th><th>Recall</th><th>Precision</th><th>False Negatives</th><th>F1-Score</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>0.00</td><td>1.00</td><td>0.2128</td><td>0</td><td>0.1754</td></tr>
                <tr><td>0.05</td><td>0.82</td><td>0.3761</td><td>9</td><td>0.6341</td></tr>
                <tr><td>0.10</td><td>0.74</td><td>0.4512</td><td>13</td><td>0.6945</td></tr>
                <tr><td>0.20</td><td>0.64</td><td>0.5517</td><td>18</td><td>0.7355</td></tr>
                <tr><td>0.50</td><td>0.44</td><td>0.6875</td><td>28</td><td>0.7193</td></tr>
                <tr><td>0.95</td><td>0.14</td><td>0.7778</td><td>43</td><td>0.5639</td></tr>
            </tbody>
        </table>
    </div>
    <p class="centered-paragraph">
       ____
    </p>


    <!-- XGBoost Classifier Detailed Results -->
    <h2 style="text-align: center;">XGBoost Classifier Results</h2>
    <div class="table-container">
        <table>
            <tr><th colspan="2">XGBoost Evaluation</th></tr>
            <tr><td>ROC AUC Score</td><td>0.8082</td></tr>
            <tr><td>Matthews Correlation Coefficient (MCC)</td><td>0.4937</td></tr>
            <tr><td>Balanced Accuracy</td><td>0.7735</td></tr>
            <tr><td>Recall (positive=1)</td><td>0.7200</td></tr>
            <tr><td>F2 Score</td><td>0.6716</td></tr>
            <tr><td>False Negatives</td><td>14</td></tr>
        </table>
    </div>

    <div class="table-container">
        <table>
            <caption>XGBoost Classification Report</caption>
            <thead>
                <tr>
                    <th></th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>0 (No Risk)</td><td>0.92</td><td>0.83</td><td>0.87</td><td>185</td></tr>
                <tr><td>1 (At Risk)</td><td>0.53</td><td>0.72</td><td>0.61</td><td>50</td></tr>
                <tr><td>Macro Avg</td><td>0.72</td><td>0.77</td><td>0.74</td><td>235</td></tr>
                <tr><td>Weighted Avg</td><td>0.83</td><td>0.80</td><td>0.81</td><td>235</td></tr>
                <tr><td>Accuracy</td><td colspan="4">0.80</td></tr>
            </tbody>
        </table>
    </div>

    <div class="table-container">
        <table>
            <caption>XGBoost Threshold Comparison</caption>
            <thead>
                <tr>
                    <th>Threshold</th><th>Recall</th><th>Precision</th><th>False Negatives</th><th>F1-Score</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>0.00</td><td>1.00</td><td>0.2128</td><td>0</td><td>0.1754</td></tr>
                <tr><td>0.10</td><td>0.94</td><td>0.2423</td><td>3</td><td>0.3608</td></tr>
                <tr><td>0.25</td><td>0.86</td><td>0.3282</td><td>7</td><td>0.5732</td></tr>
                <tr><td>0.40</td><td>0.72</td><td>0.4800</td><td>14</td><td>0.7112</td></tr>
                <tr><td>0.55</td><td>0.64</td><td>0.5614</td><td>18</td><td>0.7398</td></tr>
                <tr><td>0.95</td><td>0.04</td><td>0.6667</td><td>48</td><td>0.4790</td></tr>
            </tbody>
        </table>
    </div>
    <p class="centered-paragraph">
        _____
    </p>

    </main>
    

    <!-- Footer Section -->
    <footer>
        <p>&copy; 2025 Matthew Plambeck</p>
    </footer>

</body>
</html>


<!-- Old HTML Code, used to reference how tables were made

</p>
    <div class="table-container">
        Logistic Regression Results 
        <table>
            <tr><th colspan="2">Logistic Regression</th></tr>
            <tr><td>Train Score (Mean Accuracy)</td><td>0.811965811965812</td></tr>
            <tr><td>Test Score (Mean Accuracy)</td><td>0.8068181818181818</td></tr>
            <tr><td>Mean Squared Error</td><td>0.19318181818181818</td></tr>
            <tr><td>K-Fold Train Score</td><td>0.8071378962697434</td></tr>
            <tr><td>K-Fold Test Score</td><td>0.8075365726227796</td></tr>
        </table>

        Random Forest Results
        <table>
            <tr><th colspan="2">Random Forest Classifier</th></tr>
            <tr><td>Train Score (Mean Accuracy)</td><td>1.0</td></tr>
            <tr><td>Test Score (Mean Accuracy)</td><td>0.8579545454545454</td></tr>
            <tr><td>Mean Squared Error</td><td>0.14204545454545456</td></tr>
            <tr><td>K-Fold Train Score</td><td>1.0</td></tr>
            <tr><td>K-Fold Test Score</td><td>0.8370950888192269</td></tr>
        </table>

        Gradient Boosting Results 
        <table>
            <tr><th colspan="2">Gradient Boosting</th></tr>
            <tr><td>Train Score (Mean Accuracy)</td><td>0.9487179487179487</td></tr>
            <tr><td>Test Score (Mean Accuracy)</td><td>0.8693181818181818</td></tr>
            <tr><td>Mean Squared Error</td><td>0.13068181818181818</td></tr>
            <tr><td>K-Fold Train Score</td><td>0.9383702731680776</td></tr>
            <tr><td>K-Fold Test Score</td><td>0.8336729362591433</td></tr>
        </table>
    </div>

    <div class="table-container">
        Random Forest Classifier Report
        <table border="1">
            <tr><th colspan="5">Random Forest Classification Report</th></tr>
            <tr><th></th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>
            <tr><td>0 (No Risk)</td><td>0.87</td><td>0.97</td><td>0.92</td><td>141</td></tr>
            <tr><td>1 (At Risk)</td><td>0.78</td><td>0.40</td><td>0.53</td><td>35</td></tr>
            <tr><td>Macro Avg</td><td>0.82</td><td>0.69</td><td>0.72</td><td>176</td></tr>
            <tr><td>Weighted Avg</td><td>0.85</td><td>0.86</td><td>0.84</td><td>176</td></tr>
            <tr><td>Accuracy</td><td colspan="4">0.86 (176 total)</td></tr>
        </table>
        
        Logistic Regression Classification Report 
        <table border="1">
            <tr><th colspan="5">Logistic Regression Classification Report</th></tr>
            <tr><th></th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>
            <tr><td>0 (No Risk)</td><td>0.81</td><td>0.99</td><td>0.89</td><td>141</td></tr>
            <tr><td>1 (At Risk)</td><td>0.60</td><td>0.09</td><td>0.15</td><td>35</td></tr>
            <tr><td>Macro Avg</td><td>0.71</td><td>0.54</td><td>0.52</td><td>176</td></tr>
            <tr><td>Weighted Avg</td><td>0.77</td><td>0.81</td><td>0.74</td><td>176</td></tr>
            <tr><td>Accuracy</td><td colspan="4">0.81 (176 total)</td></tr>
        </table>
    
        Gradient Boosting Classification Report
        <table border="1">
            <tr><th colspan="5">Gradient Boosting Classifier Classification Report</th></tr>
            <tr><th></th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>
            <tr><td>0 (No Risk)</td><td>0.88</td><td>0.97</td><td>0.92</td><td>141</td></tr>
            <tr><td>1 (At Risk)</td><td>0.80</td><td>0.46</td><td>0.58</td><td>35</td></tr>
            <tr><td>Macro Avg</td><td>0.84</td><td>0.71</td><td>0.75</td><td>176</td></tr>
            <tr><td>Weighted Avg</td><td>0.86</td><td>0.87</td><td>0.85</td><td>176</td></tr>
            <tr><td>Accuracy</td><td colspan="4">0.87 (176 total)</td></tr>
        </table>
    </div>
    
    <p class="centered-paragraph">
        The results indicate classification to be the better approach. Predicting an exact sink percentage is simply too unrealistic and 
        arguably not very valuable in a real world setting. The best classifier, currently, the the gradient boosting classifier with a 
        recall score of 0.46 for at risk convoys and a precision score of 0.88 for no risk convoys. These two metrics, I deem, are the most
        import in protecting convoys. My aim is the maximize the number of true positives for convoys at risk and minimize the number false positives
        for convoys at no risk. In the real world context of convoys, lives, ships, and cargo at are stake and thus, correctly predicting convoys at 
        risk is the most important. The instances of false positives for convoys at risk (precision for at risk), although not ideal, lends to
        a level of caution in protecting convoys. If a convoy may be at risk, than, I believe, it is better to classify it as an at risk convoy
        than not as a precautionary measure. Although, in the real world, this approach may be limited/unrealistic if a finite number of resources
        (escort ships, air-cover, etc) can be allocated to convoys and thus only convoys truly at risk can afford to have more protection resources. 
        Regardless, precision and recall for convoys at risk and not a risk will be optimized in further refinements of the classifiers. The link 
        to the code for all of the results above (regressors and classifications) can be found 
        <a href="https://github.com/MAP9900/Modeling-The-Convoy-System/blob/main/Code/Machine%20Learning/Convoy_Model_2.ipynb" target="_blank">here</a>. <br>
        
    </p> -->
