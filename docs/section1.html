<!--Convoy Machine Learning    -->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Convoy Machine Learning</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to external CSS file -->
</head>
<body>
    
    <!-- Header Section -->
    <header>
        <h1>Convoy Machine Learning</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li> <!-- Link back to homepage -->
                <li><a href="section6.html">Results</a></li> <!-- Link to Section 6 Page -->
                <li><a href="section2.html">Convoy Data Visualization</a></li> <!-- Link to Section 2 Page -->
                <li><a href="section3.html">Convoy History</a></li> <!-- Link to Section 3 Page -->
                <li><a href="section4.html">Original Project</a></li> <!-- Link to Section 4 Page -->
                <li><a href="section5.html">Additional Info</a></li> <!-- Link to Section 5 Page -->
            </ul>
        </nav>
    </header>

    <main>
        
<!-- Original Classification Results -->
        <h2 style="text-align: center;">Exploring Classification Algorithms</h2>
        <p class="centered-paragraph">
        

        Early experiments utilized a regression approach to predict exact sink percentages. The results for these initial attempts 
        can be found <a href="section1_1.html">here.</a> The limited data, wide variability, and unmodeled factors, however, made this
        approach unsuitable for the problem. Exact sink percentages of each convoy proved impossible to predict within the scope
        of the project. Instead, sink percentages were converted to binary values of 0 (no ships sunk) or 1 (at least one ship sunk). 
        This new method, based on classification, aims to alleviate the complexity of predicting an exact sink percentage while also
        providing a more realistic approach of identifying high-risk convoys. Various algorithms were tested to determine the one 
        best suited for the challenge.
        </p>
        
        <div class="table-container">
            <table>
                <caption>Classification Algorithm Comparison:</caption>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Acc</th>
                        <th>ROC_AUC</th>
                        <th>MCC</th>
                        <th>Bal_Acc</th>
                        <th>Recall1</th>
                        <th>F1_1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>LogisticRegression</td><td>0.79</td><td>0.786</td><td>0.193</td><td>0.554</td><td>0.14</td><td>0.22</td></tr>
                    <tr><td>SGDClassifier</td><td>0.79</td><td>0.728</td><td>0.148</td><td>0.537</td><td>0.10</td><td>0.17</td></tr>
                    <tr><td>LinearSVC</td><td>0.79</td><td>0.535</td><td>0.000</td><td>0.500</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>SVC</td><td>0.80</td><td>0.768</td><td>0.215</td><td>0.545</td><td>0.10</td><td>0.18</td></tr>
                    <tr><td>NuSVC</td><td>0.77</td><td>0.667</td><td>0.241</td><td>0.606</td><td>0.32</td><td>0.37</td></tr>
                    <tr><td>KNeighborsClassifier</td><td>0.80</td><td>0.740</td><td>0.265</td><td>0.588</td><td>0.22</td><td>0.32</td></tr>
                    <tr><td>RadiusNeighborsClassifier</td><td>0.78</td><td>0.567</td><td>-0.034</td><td>0.497</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>DecisionTreeClassifier</td><td>0.74</td><td>0.621</td><td>0.235</td><td>0.621</td><td>0.42</td><td>0.40</td></tr>
                    <tr><td>RandomForestClassifier</td><td>0.84</td><td>0.815</td><td>0.461</td><td>0.674</td><td>0.38</td><td>0.51</td></tr>
                    <tr><td>ExtraTreesClassifier</td><td>0.84</td><td>0.791</td><td>0.469</td><td>0.688</td><td>0.42</td><td>0.53</td></tr>
                    <tr><td>BaggingClassifier</td><td>0.83</td><td>0.766</td><td>0.400</td><td>0.656</td><td>0.36</td><td>0.47</td></tr>
                    <tr class="highlight-row"><td>GradientBoostingClassifier</td><td>0.84</td><td>0.835</td><td>0.473</td><td>0.696</td><td>0.44</td><td>0.54</td></tr>
                    <tr><td>AdaBoostClassifier</td><td>0.82</td><td>0.790</td><td>0.368</td><td>0.631</td><td>0.30</td><td>0.42</td></tr>
                    <tr><td>GaussianNB</td><td>0.77</td><td>0.670</td><td>0.096</td><td>0.530</td><td>0.12</td><td>0.18</td></tr>
                    <tr><td>BernoulliNB</td><td>0.79</td><td>0.699</td><td>0.000</td><td>0.500</td><td>0.00</td><td>0.00</td></tr>
                    <tr><td>ComplementNB</td><td>0.49</td><td>0.555</td><td>0.061</td><td>0.537</td><td>0.62</td><td>0.34</td></tr>
                    <tr><td>LinearDiscriminantAnalysis</td><td>0.79</td><td>0.787</td><td>0.205</td><td>0.561</td><td>0.16</td><td>0.25</td></tr>
                    <tr><td>QuadraticDiscriminantAnalysis</td><td>0.78</td><td>0.790</td><td>0.425</td><td>0.735</td><td>0.66</td><td>0.56</td></tr>
                    <tr><td>MLPClassifier</td><td>0.67</td><td>0.542</td><td>-0.064</td><td>0.471</td><td>0.12</td><td>0.13</td></tr>
                </tbody>
            </table>
        </div>
        <p class="centered-paragraph">
        The Gradient Boosting Classifier performs the best on this data and will be the foundation for all future tests. 
        Notably, the RandomForestClassifier, QuadraticDiscriminantAnalysis, and ComplementNB algorithms performed comparably, even resulting in higher 
        recall scores. However, ComplementNB is an algorithm developed for text classification and is best suited for imbalanced
        and discrete datasets. Tests done in this 
        <a href="https://github.com/MAP9900/Convoy-Predictor-Project/blob/main/notebooks/exploration/CNB_and_QDA_Test.ipynb" target="_blank">notebook</a>
        reveal the ComplementNB algorithm fails to produce meaningful results. The model results in near binary behavior; 
        at low thresholds everything is classified as positive and at high thresholds, everything is classified as negative. 
        Other Gradient Boosting algorithms from the XGBoost library will be tested next. These results are below.


        </p>

        <div class="table-container">
            <table>
                <caption>Gradient Boosting Family Comparison:</caption>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Acc</th>
                        <th>ROC_AUC</th>
                        <th>MCC</th>
                        <th>Bal_Acc</th>
                        <th>Recall1</th>
                        <th>F1_1</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight-row"><td>GradientBoostingClassifier</td><td>0.84</td><td>0.835</td><td>0.473</td><td>0.696</td><td>0.44</td><td>0.54</td></tr>
                    <tr><td>XGBClassifier</td><td>0.83</td><td>0.773</td><td>0.408</td><td>0.651</td><td>0.34</td><td>0.46</td></tr>
                    <tr><td>LGBMClassifier</td><td>0.83</td><td>0.792</td><td>0.443</td><td>0.683</td><td>0.42</td><td>0.52</td></tr>
                    <tr><td>CatBoostClassifier</td><td>0.84</td><td>0.832</td><td>0.457</td><td>0.659</td><td>0.34</td><td>0.48</td></tr>
                </tbody>
            </table>
        </div>

        <p class="centered-paragraph">
            The standard Scikit-Learn Gradient Boosting model outperforms the other boosted tree variants, achieving the highest 
            ROC-AUC and F1 scores along with the strongest recall for at-risk convoys. Further optimization will focus on 
            improving recall and reducing false negatives. The emphasis on recall (true positives divided by true positives plus 
            false negatives) is necessary because identifying convoys at risk is more important than identifying those not at risk.
            False negatives (cases where the model predicts a convoy is not at risk when it actually is) are particularly dangerous, 
            especially with human life and vital wartime supplies at stake. False positives, while not ideal, carry much less
            associated risk. If a convoy is labeled as at risk but in reality is not, no harm results. There are limitations to
            false negatives, such as the need to allocate defensive resources; otherwise, every convoy could simply be labeled at risk.
            However, some false positives are acceptable as a form of caution.
        </p>

        <!-- Gradient Boosting Classifier Detailed Results -->



    <h2 style="text-align: center;">Optimizing the Gradient Boosting Model and Comparing Thresholds</h2>

     <p class="centered-paragraph">
        To optimize the gradient boosting classifier, Grid Search Cross Validation will be used to test a predefined set of hyperparameters. The hyperparameters 
        optimized are learning_rate, n_estimators, max_depth, min_samples_leaf, min_samples_split, subsample, and max_features. To learn more about these hyperparameters
        see Scikit-Learn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" target="_blank">docs</a>. 
        After Grid Search CV, a threshold comparison is done to examine the effect on recall, precision, and accuracy. The threshold, which defaults to 0.5, is the value used to 
        convert the model's probability output into a discrete class (0 or 1). With the default value of 0.5, any output below 0.5 will be classified as 0 and any output above 0.5 will be 1.
        By manipulating the threshold, more weight can be placed on recall and limiting the number of false negatives at the expense of decreasing precision and increasing the number of
        false positives. It is also worth noting that overall accuracy has decreased from the algorithm comparison test, but this is easily explained by the optimization techniques 
        employed, which prioritized recall and F1-score over accuracy. The results for the optimized classifier and threshold comparison are below. 

        </p>
        <br>
        <section class="table-section">
            <!-- <h3>Threshold Comparison</h3> -->
            <div class="table-block table-pair">
                <table>
                    <caption>Optimized GB Classifier Threshold Comparison (0.00 – 0.50)</caption>
                    <thead>
                        <tr>
                            <th>Threshold</th>
                            <th>Recall</th>
                            <th>Precision</th>
                            <th>False Negatives</th>
                            <th>Accuracy</th>
                            <th>F1-Score</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>0.00</td><td>1.00</td><td>0.212766</td><td>0</td><td>0.212766</td><td>0.175439</td></tr>
                        <tr><td>0.05</td><td>0.86</td><td>0.361345</td><td>7</td><td>0.646809</td><td>0.616564</td></tr>
                        <tr><td>0.10</td><td>0.80</td><td>0.449438</td><td>10</td><td>0.748936</td><td>0.698646</td></tr>
                        <tr><td>0.15</td><td>0.72</td><td>0.480000</td><td>14</td><td>0.774468</td><td>0.711188</td></tr>
                        <tr><td>0.20</td><td>0.68</td><td>0.523077</td><td>16</td><td>0.800000</td><td>0.729455</td></tr>
                        <tr><td>0.25</td><td>0.64</td><td>0.551724</td><td>18</td><td>0.812766</td><td>0.735523</td></tr>
                        <tr><td>0.30</td><td>0.64</td><td>0.592593</td><td>18</td><td>0.829787</td><td>0.753047</td></tr>
                        <tr><td>0.35</td><td>0.60</td><td>0.600000</td><td>20</td><td>0.829787</td><td>0.745946</td></tr>
                        <tr><td>0.40</td><td>0.54</td><td>0.600000</td><td>23</td><td>0.825532</td><td>0.729544</td></tr>
                        <tr><td>0.45</td><td>0.46</td><td>0.575000</td><td>27</td><td>0.812766</td><td>0.697661</td></tr>
                        <tr><td>0.50</td><td>0.42</td><td>0.567568</td><td>29</td><td>0.808511</td><td>0.682633</td></tr>
                    </tbody>
                </table>
                <table>
                    <caption>Optimized GB Classifier Threshold Comparison (0.55 – 1.00)</caption>
                    <thead>
                        <tr>
                            <th>Threshold</th>
                            <th>Recall</th>
                            <th>Precision</th>
                            <th>False Negatives</th>
                            <th>Accuracy</th>
                            <th>F1-Score</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>0.55</td><td>0.40</td><td>0.555556</td><td>30</td><td>0.804255</td><td>0.672662</td></tr>
                        <tr><td>0.60</td><td>0.36</td><td>0.620690</td><td>32</td><td>0.817021</td><td>0.672861</td></tr>
                        <tr><td>0.65</td><td>0.34</td><td>0.653846</td><td>33</td><td>0.821277</td><td>0.670385</td></tr>
                        <tr><td>0.70</td><td>0.32</td><td>0.640000</td><td>34</td><td>0.817021</td><td>0.658903</td></tr>
                        <tr><td>0.75</td><td>0.32</td><td>0.695652</td><td>34</td><td>0.825532</td><td>0.667541</td></tr>
                        <tr><td>0.80</td><td>0.30</td><td>0.714286</td><td>35</td><td>0.825532</td><td>0.659889</td></tr>
                        <tr><td>0.85</td><td>0.24</td><td>0.705882</td><td>38</td><td>0.817021</td><td>0.625755</td></tr>
                        <tr><td>0.90</td><td>0.14</td><td>0.700000</td><td>43</td><td>0.804255</td><td>0.560569</td></tr>
                        <tr><td>0.95</td><td>0.12</td><td>0.857143</td><td>44</td><td>0.808511</td><td>0.550784</td></tr>
                        <tr><td>1.00</td><td>0.00</td><td>0.000000</td><td>50</td><td>0.787234</td><td>0.440476</td></tr>
                    </tbody>
                </table>
            </div>
        </section>

        <figure>
                <h4>Visualization of Different Thresholds</h4>
                <img src="images/Optimized_GB_Threshold_Results.png" alt="Visualization of Different Thresholds" width="750" height="500">
                <!-- <figcaption>____</figcaption> -->
            </figure>

     <p class="centered-paragraph">
        The threshold comparison results indicate a threshold interval of [0.25, 0.35] to be the best-performing. At a threshold of 0.30, F1-score peaks at 
        0.75 and so does accuracy at 0.83, with the best balance between recall and precision. However, the number of false negatives remains relatively high
        at 18 convoys classified as not a risk despite having ships sunk. A recall-focused interval is instead [0.05, 0.15]. At this interval,
        false negatives are 7, 10, and 14, respectively. These results are more in line with the goal of producing a model with a high recall score. Accuracy and F1-score 
        remain high at a threshold of 0.15 but quickly decline at thresholds of 0.10 and 0.05. Therefore, determining a single, optimal threshold is difficult, with each value 
        having trade-offs between the different metrics. A good balance, however, is a threshold of 0.20, which sees recall, accuracy, and F1-score 
        remain high at 0.68, 0.80, and 0.73, respectively, and only 16 false negatives occurring. 
     </p>
    
    <h2 style="text-align: center;">Exploring Other Promising Algorithms</h2>
    <p class="centered-paragraph">
        Prior tests and the results above indicate multiple algorithms with the potential to produce strong results. These other
        models will now be optimized and tested to compare with the GradientBoostingClassifier. Optimization is done through
        Halving Grid-Search on medium-sized parameter grids. This will leave room for further, more in-depth optimization
        but allow for solid baseline triaging beyond the simple algorithm test. The best resulting models will move on to 
        further optimization and potential use in a voting-based ensemble model. 

    </p>
    <h4 style="text-align: center;">Optimized Results</h4>

    <div class="table-container">
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>Acc</th>
                <th>ROC_AUC</th>
                <th>MCC</th>
                <th>Bal_Acc</th>
                <th>Recall1</th>
                <th>F1_1</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>DecisionTreeClassifier</td><td>0.57</td><td>0.7449</td><td>0.2751</td><td>0.6659</td><td>0.84</td><td>0.45</td></tr>
            <tr><td>RandomForestClassifier</td><td>0.82</td><td>0.8128</td><td>0.5145</td><td>0.7770</td><td>0.70</td><td>0.62</td></tr>
            <tr><td>ExtraTreesClassifier</td><td>0.83</td><td>0.8123</td><td>0.5156</td><td>0.7632</td><td>0.64</td><td>0.62</td></tr>
            <tr><td>BaggingClassifier</td><td>0.77</td><td>0.8214</td><td>0.4310</td><td>0.7446</td><td>0.70</td><td>0.56</td></tr>
            <tr><td>GradientBoostingClassifier</td><td>0.82</td><td>0.8036</td><td>0.4033</td><td>0.6795</td><td>0.44</td><td>0.51</td></tr>
            <tr><td>AdaBoostClassifier</td><td>0.61</td><td>0.7818</td><td>0.3334</td><td>0.7030</td><td>0.86</td><td>0.49</td></tr>
            <tr><td>QuadraticDiscriminantAnalysis</td><td>0.60</td><td>0.7897</td><td>0.3335</td><td>0.7022</td><td>0.88</td><td>0.48</td></tr>
            <tr><td>XGBClassifier</td><td>0.80</td><td>0.7670</td><td>0.3725</td><td>0.6759</td><td>0.46</td><td>0.49</td></tr>
            <tr><td>XGBRFClassifier</td><td>0.69</td><td>0.7942</td><td>0.3506</td><td>0.7105</td><td>0.74</td><td>0.51</td></tr>
            <tr><td>LGBMClassifier</td><td>0.77</td><td>0.7469</td><td>0.3437</td><td>0.6789</td><td>0.52</td><td>0.49</td></tr>
            <tr><td>CatBoostClassifier</td><td>0.74</td><td>0.7717</td><td>0.3681</td><td>0.7111</td><td>0.66</td><td>0.52</td></tr>
        </tbody>
    </table>
</div>
    <p class="centered-paragraph">
        Recall-optimized tuning now results in recall scores in the 0.60-0.80 range for the tree ensemble algorithms and 
        the QuadraticDiscriminantAnalysis model but at the expense of decreases in accuracy, MCC, and precision.
        Pushing recall past the 0.70 mark drastically worsens this trade-off, causing collapses in MCC and accuracy.
        Boosting-based models exhibit weaker recall responsiveness under recall-optimized tuning, suggesting threshold or 
        loss-function misalignment rather than insufficient model capacity. 

        These results indicate that more than just the GradientBoostingClassifier can perform well on this data, motivating 
        the use of ensemble models to capture predictive nuance that may be lost when relying on a single classifier. 
        Despite extensive recall-focused optimization, the models still exhibit room for improvement, as evidenced by recall plateaus, 
        degraded decision quality at high-recall operating points, and overlapping error patterns that ensemble methods may help mitigate.
    
    </p>

<h2 style="text-align: center;">Ensemble Model Results</h2>
<div class="table-container">
    <table>
        <!-- <caption>Ensemble Model Comparison:</caption> -->
        <thead>
            <tr>
                <th>Model</th>
                <th>Acc</th>
                <th>ROC_AUC</th>
                <th>MCC</th>
                <th>Bal_Acc</th>
                <th>Recall1</th>
                <th>Precision1</th>
                <th>F1_1</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Best Recall</td><td>0.796</td><td>0.817</td><td>0.462</td><td>0.754</td><td>0.680</td><td>0.515</td><td>0.586</td></tr>
            <tr><td>Best Balance</td><td>0.843</td><td>0.823</td><td>0.520</td><td>0.754</td><td>0.600</td><td>0.638</td><td>0.619</td></tr>
            <tr><td>Middle Performers</td><td>0.834</td><td>0.810</td><td>0.501</td><td>0.749</td><td>0.600</td><td>0.612</td><td>0.606</td></tr>
            <tr><td>Five Model Ensemble</td><td>0.826</td><td>0.836</td><td>0.506</td><td>0.765</td><td>0.660</td><td>0.579</td><td>0.617</td></tr>
            <tr><td>Six Model Ensemble</td><td>0.796</td><td>0.833</td><td>0.453</td><td>0.746</td><td>0.660</td><td>0.516</td><td>0.579</td></tr>
            <tr><td>Six Model Ensemble (2)</td><td>0.830</td><td>0.841</td><td>0.507</td><td>0.761</td><td>0.640</td><td>0.593</td><td>0.615</td></tr>

        </tbody>
    </table>
</div>
    <p class="centered-paragraph">
        Five combinations of soft voting ensemble models were tested with the results above.
        Best Recall takes the top three recall performers from the optimized results above 
        (QuadraticDiscriminantAnalysis, AdaBoostClassifier, and DecisionTreeClassifier). Best Balance takes three models with strong ROC AUC, MCC, and balanced accuracy along 
        with decent recall (RandomForestClassifier, ExtraTreesClassifier, and GradientBoostingClassifier). Best Mid takes three decently recall-performing models and acts as 
        a comparison ensemble model (XGBRFClassifier, CatBoostClassifier, and BaggingClassifier). Five-model ensemble combines Best Recall and part of Best Balance 
        (QuadraticDiscriminantAnalysis, AdaBoostClassifier, DecisionTreeClassifier, RandomForestClassifier, and ExtraTreesClassifier). Six Model Ensemble adds the 
        XGBRFClassifier to the Five Model Ensemble. Six Model Ensemble (2) is all models from Best Recall and Best Balance. 
        
        <br><br>
        Initial ensemble metrics demonstrate clear performance gains relative to the individual models. Ensemble accuracies reach 0.84, exceeding the highest
        accuracy achieved by any single classifier (ExtraTreesClassifier), while ROC-AUC and MCC also improve compared to the top-performing individual models.
        Although no single ensemble dominates across all metrics, the five-model ensemble consistently provides the strongest overall trade-off between recall, 
        discriminative ability, and decision quality, suggesting diminishing returns from adding additional models. Further evaluation should focus on more 
        granular comparisons between ensemble configurations and on alternative voting strategies, including weighted and threshold-aware approaches, to determine 
        whether recall can be further improved without degrading overall model reliability.
    </p>
        TODO: Add five vs six model comparison



    </main>
    

    <!-- Footer Section -->
    <footer>
        <p>&copy; 2025 Matthew Plambeck</p>
    </footer>

</body>
</html>





















<!-- 

--- Results of Algorithm_Test_3.ipynb --- 

--- Old Results ---
        <tbody>
            <tr><td>DecisionTreeClassifier</td><td>0.71</td><td>0.7449</td><td>0.3337</td><td>0.6949</td><td>0.66</td><td>0.50</td></tr>
            <tr><td>RandomForestClassifier</td><td>0.84</td><td>0.8128</td><td>0.5199</td><td>0.7541</td><td>0.60</td><td>0.62</td></tr>
            <tr><td>ExtraTreesClassifier</td><td>0.86</td><td>0.8123</td><td>0.5662</td><td>0.7722</td><td>0.62</td><td>0.65</td></tr>
            <tr><td>BaggingClassifier</td><td>0.82</td><td>0.8214</td><td>0.4739</td><td>0.7451</td><td>0.62</td><td>0.59</td></tr>
            <tr><td>GradientBoostingClassifier</td><td>0.82</td><td>0.8036</td><td>0.4033</td><td>0.6795</td><td>0.44</td><td>0.51</td></tr>
            <tr><td>AdaBoostClassifier</td><td>0.81</td><td>0.7818</td><td>0.4578</td><td>0.7397</td><td>0.62</td><td>0.58</td></tr>
            <tr><td>QuadraticDiscriminantAnalysis</td><td>0.77</td><td>0.7897</td><td>0.4374</td><td>0.7473</td><td>0.70</td><td>0.57</td></tr>
            <tr><td>XGBClassifier</td><td>0.81</td><td>0.7670</td><td>0.3585</td><td>0.6522</td><td>0.38</td><td>0.46</td></tr>
            <tr><td>XGBRFClassifier</td><td>0.80</td><td>0.7942</td><td>0.4586</td><td>0.7443</td><td>0.64</td><td>0.58</td></tr>
            <tr><td>LGBMClassifier</td><td>0.82</td><td>0.7469</td><td>0.3474</td><td>0.6211</td><td>0.28</td><td>0.39</td></tr>
            <tr><td>CatBoostClassifier</td><td>0.81</td><td>0.7717</td><td>0.3748</td><td>0.6668</td><td>0.42</td><td>0.48</td></tr>
        </tbody>

--- New Results --- 
--- Threshold Calculation Changed, optimization stayed the same --- 

        <tbody>
            <tr><td>DecisionTreeClassifier</td><td>0.57</td><td>0.7449</td><td>0.2751</td><td>0.6659</td><td>0.84</td><td>0.45</td></tr>
            <tr><td>RandomForestClassifier</td><td>0.82</td><td>0.8128</td><td>0.5145</td><td>0.7770</td><td>0.70</td><td>0.62</td></tr>
            <tr><td>ExtraTreesClassifier</td><td>0.83</td><td>0.8123</td><td>0.5156</td><td>0.7632</td><td>0.64</td><td>0.62</td></tr>
            <tr><td>BaggingClassifier</td><td>0.77</td><td>0.8214</td><td>0.4310</td><td>0.7446</td><td>0.70</td><td>0.56</td></tr>
            <tr><td>GradientBoostingClassifier</td><td>0.82</td><td>0.8036</td><td>0.4033</td><td>0.6795</td><td>0.44</td><td>0.51</td></tr>
            <tr><td>AdaBoostClassifier</td><td>0.61</td><td>0.7818</td><td>0.3334</td><td>0.7030</td><td>0.86</td><td>0.49</td></tr>
            <tr><td>QuadraticDiscriminantAnalysis</td><td>0.60</td><td>0.7897</td><td>0.3335</td><td>0.7022</td><td>0.88</td><td>0.48</td></tr>
            <tr><td>XGBClassifier</td><td>0.80</td><td>0.7670</td><td>0.3725</td><td>0.6759</td><td>0.46</td><td>0.49</td></tr>
            <tr><td>XGBRFClassifier</td><td>0.69</td><td>0.7942</td><td>0.3506</td><td>0.7105</td><td>0.74</td><td>0.51</td></tr>
            <tr><td>LGBMClassifier</td><td>0.77</td><td>0.7469</td><td>0.3437</td><td>0.6789</td><td>0.52</td><td>0.49</td></tr>
            <tr><td>CatBoostClassifier</td><td>0.74</td><td>0.7717</td><td>0.3681</td><td>0.7111</td><td>0.66</td><td>0.52</td></tr>
        </tbody>



-->
