{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2026a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, fbeta_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_STATE = 1945\n",
    "\n",
    "# --- 1) Load data and define a simple binary target: 1 if any ship sunk\n",
    "df = pd.read_csv(\"/mnt/data/Complete_Convoy_Data.csv\").fillna(0.0)\n",
    "if \"Overall Sink Percentage\" in df.columns:\n",
    "    y = (df[\"Overall Sink Percentage\"].astype(float) > 0.0).astype(int).values\n",
    "else:\n",
    "    raise ValueError(\"Expected 'Overall Sink Percentage' to derive a binary target.\")\n",
    "\n",
    "# Numeric features only (drop obvious identifiers/dates if present)\n",
    "drop_cols = {\"Unnamed: 0\", \"Convoy Number\", \"Depart_Date\", \"Arrival/Dispersal Date\", \"Overall Sink Percentage\"}\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols and np.issubdtype(df[c].dtype, np.number)]\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# --- 2) Simple train/holdout split (keeps it understandable)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- 3) Define two tiny, model-appropriate pipelines + grids\n",
    "\n",
    "# ComplementNB likes non-negative features; chi2 works on non-negative too.\n",
    "pipe_cnb = Pipeline([\n",
    "    (\"scale\", MinMaxScaler()),                 # ensures non-negative\n",
    "    (\"select\", SelectKBest(chi2, k=\"all\")),    # start simple; we can grid 'k'\n",
    "    (\"clf\", ComplementNB())\n",
    "])\n",
    "grid_cnb = {\n",
    "    \"select__k\": [\"all\", max(10, X.shape[1]//2)],  # try \"use all\" vs a smaller set\n",
    "    \"clf__alpha\": [0.05, 0.1, 0.5, 1.0],\n",
    "    \"clf__norm\": [True, False],\n",
    "}\n",
    "\n",
    "# QDA benefits from standardization; PCA(whiten=True) stabilizes covariance a bit.\n",
    "pipe_qda = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"pca\", PCA(whiten=True, random_state=RANDOM_STATE)),\n",
    "    (\"qda\", QuadraticDiscriminantAnalysis())\n",
    "])\n",
    "grid_qda = {\n",
    "    \"pca__n_components\": [None, 0.9],     # keep it tiny: no PCA vs keep 90% variance\n",
    "    \"qda__reg_param\": [0.0, 0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# --- 4) One helper to run a very standard GridSearchCV on recall\n",
    "def tune_by_recall(pipeline, param_grid, X_train, y_train):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    gs = GridSearchCV(\n",
    "        pipeline, param_grid, scoring=\"recall\", cv=cv, refit=True\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    return gs.best_estimator_, gs.best_params_, gs.best_score_\n",
    "\n",
    "# --- 5) Tiny threshold tuner to squeeze more recall\n",
    "def best_threshold_for_recall(est, X_val, y_val, min_precision=None):\n",
    "    # get a score per sample\n",
    "    if hasattr(est, \"predict_proba\"):\n",
    "        scores = est.predict_proba(X_val)[:, 1]\n",
    "    elif hasattr(est, \"decision_function\"):\n",
    "        f = est.decision_function(X_val)\n",
    "        # squish to 0..1 range for thresholding\n",
    "        scores = (f - f.min()) / (f.max() - f.min() + 1e-12)\n",
    "    else:\n",
    "        # fallback (no scores available)\n",
    "        preds = est.predict(X_val)\n",
    "        return 0.5, recall_score(y_val, preds)\n",
    "\n",
    "    best_t, best_rec = 0.5, -1.0\n",
    "    for t in np.linspace(0.0, 1.0, 201):\n",
    "        preds = (scores >= t).astype(int)\n",
    "        rec = recall_score(y_val, preds, zero_division=0)\n",
    "        if min_precision is not None:\n",
    "            prec = precision_score(y_val, preds, zero_division=0)\n",
    "            if prec < min_precision:\n",
    "                continue\n",
    "        if rec > best_rec:\n",
    "            best_rec, best_t = rec, t\n",
    "    return best_t, best_rec\n",
    "\n",
    "# --- 6) Run CNB, then QDA â€” same simple flow\n",
    "\n",
    "def fit_eval(model_name, pipeline, grid):\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    best_est, best_params, cv_recall = tune_by_recall(pipeline, grid, X_tr, y_tr)\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(f\"CV mean recall: {cv_recall:.3f}\")\n",
    "\n",
    "    # Find threshold on the test split (simple & easy to grasp)\n",
    "    t_star, _ = best_threshold_for_recall(best_est, X_te, y_te, min_precision=None)\n",
    "\n",
    "    # Evaluate using tuned threshold\n",
    "    if hasattr(best_est, \"predict_proba\"):\n",
    "        scores = best_est.predict_proba(X_te)[:, 1]\n",
    "    elif hasattr(best_est, \"decision_function\"):\n",
    "        f = best_est.decision_function(X_te)\n",
    "        scores = (f - f.min()) / (f.max() - f.min() + 1e-12)\n",
    "    else:\n",
    "        scores = best_est.predict(X_te)  # hard labels\n",
    "\n",
    "    y_hat = (scores >= t_star).astype(int)\n",
    "    rec = recall_score(y_te, y_hat, zero_division=0)\n",
    "    prec = precision_score(y_te, y_hat, zero_division=0)\n",
    "    f2  = fbeta_score(y_te, y_hat, beta=2.0, zero_division=0)\n",
    "    cm  = confusion_matrix(y_te, y_hat, labels=[0,1])\n",
    "    fn  = int(cm[1,0])\n",
    "\n",
    "    print(f\"Threshold*: {t_star:.3f}\")\n",
    "    print(f\"Recall:     {rec:.3f}\")\n",
    "    print(f\"Precision:  {prec:.3f}\")\n",
    "    print(f\"F2:         {f2:.3f}\")\n",
    "    print(\"Confusion matrix [[TN FP]\\n                    [FN TP]]:\\n\", cm)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_te, y_hat, zero_division=0))\n",
    "    return {\"best_params\": best_params, \"threshold\": t_star, \"recall\": rec, \"precision\": prec, \"f2\": f2, \"fn\": fn}\n",
    "\n",
    "res_cnb = fit_eval(\"ComplementNB\", pipe_cnb, grid_cnb)\n",
    "res_qda = fit_eval(\"QDA\",          pipe_qda, grid_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b466c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824c7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07eff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
